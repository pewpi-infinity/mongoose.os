#!/usr/bin/env python3
import os, json, random, datetime, textwrap, re

# Files / Directories
TERMS_FILE      = "cart_153_master_terms.txt"
SCRAPED_DIRS    = [
    "scraped_cart101_physics",
    "scraped_cart102_ai",
    "scraped_cart103_energy",
    "scraped_cart104_academic",
    "scraped_cart105_materials",
]
PAPERS_DIR      = "infinity_papers"
TOKENS_DIR      = "tokens_cart101_physics"
TOKENS_LOG_PATH = os.path.join(TOKENS_DIR, "token_log.json")

# ONLY READ FIRST 20KB → avoids “stuck” issue
MAX_BYTES = 20000

# Termux color palette
C = {
    "hdr":  "\033[95m",
    "ok":   "\033[92m",
    "warn": "\033[93m",
    "err":  "\033[91m",
    "info": "\033[94m",
    "reset":"\033[0m",
}

os.makedirs(PAPERS_DIR, exist_ok=True)
os.makedirs(TOKENS_DIR, exist_ok=True)

# -----------------------------
# LOAD TERMS
# -----------------------------
def load_terms():
    if not os.path.exists(TERMS_FILE):
        print(f"{C['err']}[∞] TERMS_FILE missing: {TERMS_FILE}{C['reset']}")
        return []
    terms = []
    with open(TERMS_FILE, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            t = line.strip()
            if t and not t.startswith("#"):
                terms.append(t)
    return terms

# -----------------------------
# CALCULATE TOKEN VALUE (your $90 → $3m logic)
# -----------------------------
def compute_token_value(num_bytes: int, num_links: int) -> int:
    base = 1_000_000
    size_bonus = min(num_bytes // 200, 1_000_000)
    link_bonus = min(num_links * 50_000, 1_000_000)
    raw = base + size_bonus + link_bonus
    return max(90, min(raw, 3_000_000))

# -----------------------------
# HYDROGEN COLOR LOGIC
# -----------------------------
def pick_color_from_terms(terms):
    joined = " ".join(t.lower() for t in terms)
    if "hydrogen" in joined:
        return "PURPLE"
    palette = ["RED", "GREEN", "BLUE", "YELLOW", "ORANGE", "PINK"]
    return random.choice(palette)

# -----------------------------
# FAST SCAN (NO HANG) — FIRST 20KB ONLY
# -----------------------------
def scan_scraped_sources(terms):
    hits = {t: [] for t in terms}
    for d in SCRAPED_DIRS:
        if not os.path.isdir(d):
            continue
        for fname in os.listdir(d):
            path = os.path.join(d, fname)
            if not os.path.isfile(path):
                continue

            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read(MAX_BYTES)
            except:
                continue

            # look for terms (simple + fast)
            for term in terms:
                if len(hits[term]) >= 5:
                    continue
                if term.lower() in text.lower():
                    excerpt = text.replace("\n", " ")[:240]
                    hits[term].append({
                        "source": path,
                        "excerpt": excerpt
                    })
    return hits

# -----------------------------
# LOAD / SAVE TOKEN LOG
# -----------------------------
def load_token_log():
    if not os.path.exists(TOKENS_LOG_PATH):
        return {"last_token": 0, "tokens": []}
    try:
        with open(TOKENS_LOG_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {"last_token": 0, "tokens": []}

def save_token_log(log):
    with open(TOKENS_LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(log, f, indent=2)

# -----------------------------
# USD FORMATTER ($1.5m etc.)
# -----------------------------
def format_usd_millions(value: int) -> str:
    millions = value / 1_000_000.0
    if millions < 1:
        return f"${value:,}"
    return f"${millions:.1f}m"

# -----------------------------
# BUILD THE PAPER
# -----------------------------
def build_paper_body(token_id, terms, color, citations):
    ts = datetime.datetime.now().isoformat(timespec="seconds")

    header = [
        "# ∞ Infinity Research Paper",
        "## Cart: CART102 / Category: Infinity_OS",
        f"## Token #: {token_id}",
        "",  # placeholder → we fill this with the value later
        f"## Token Color: {color}",
        f"## Generated: {ts}",
        "",
        "### Search Terms",
        ", ".join(terms),
        "",
        "---",
        "",
        "## Infinity Token Narrative",
        textwrap.fill(
            "This research paper is generated by the Infinity OS using your "
            "existing scraped archives as substrate. It does not copy external "
            "content — instead it interprets your stored data through hydrogen "
            "quantum logic, color lattices, and frequency mapping.",
            width=88,
        ),
        "",
    ]

    sections = []
    for term in terms:
        sections.append(f"## {term.title()}")
        sections.append("")
        sections.append(
            textwrap.fill(
                f"Infinity OS treats **{term}** as a hydrogen-linked node. "
                "Its color pathways inform the operating system where this concept "
                "connects inside the lattice.",
                width=88,
            )
        )
        sections.append("")
        if citations.get(term):
            sections.append(f"### Jump Links ({term})")
            for i, hit in enumerate(citations[term][:5], start=1):
                sections.append(
                    f"- Jump {i}: `{hit['source']}` — …{hit['excerpt']}…"
                )
            sections.append("")
        else:
            sections.append("No excerpts yet in scraped sources.\n")

        sections.append("---\n")

    footer = [
        "## Citations & Credits",
        textwrap.fill(
            "Jump links reference your local scraped archives only. "
            "No external content is pasted.",
            width=88,
        ),
        "",
    ]

    return "\n".join(header + sections + footer)

# -----------------------------
# MAIN
# -----------------------------
def main():
    print(f"{C['hdr']}[∞] Infinity Termux Research Writer — ONLINE{C['reset']}")

    terms = load_terms()
    if len(terms) < 4:
        print(f"{C['err']}[∞] Need at least 4 terms in {TERMS_FILE}{C['reset']}")
        return

    chosen_terms = sorted(random.sample(terms, 4))
    print(f"{C['info']}[∞] Terms → {', '.join(chosen_terms)}{C['reset']}")

    citations = scan_scraped_sources(chosen_terms)

    log = load_token_log()
    token_id = log.get("last_token", 0) + 1
    color = pick_color_from_terms(chosen_terms)

    body = build_paper_body(token_id, chosen_terms, color, citations)
    num_bytes = len(body.encode("utf-8"))
    total_links = sum(len(citations[t]) for t in chosen_terms)

    token_value = compute_token_value(num_bytes, total_links)
    usd_str = format_usd_millions(token_value)

    lines = body.splitlines()
    lines[3] = f"## Token Value: {usd_str}  (bytes={num_bytes}, links={total_links})"
    final_body = "\n".join(lines)

    slug = chosen_terms[0].replace(" ", "_")[:24]
    filename = f"PAPER_{token_id:06d}_{color}_{slug}.md"
    outpath = os.path.join(PAPERS_DIR, filename)

    with open(outpath, "w", encoding="utf-8") as f:
        f.write(final_body)

    log["last_token"] = token_id
    log["tokens"].append({
        "token_id": token_id,
        "file": outpath,
        "terms": chosen_terms,
        "color": color,
        "value": token_value,
        "usd": usd_str,
        "bytes": num_bytes,
        "links": total_links,
        "generated": datetime.datetime.now().isoformat(timespec="seconds"),
    })
    save_token_log(log)

    print(f"{C['ok']}[∞] Minted {filename}{C['reset']}")
    print(f"{C['ok']}[∞] Value → {usd_str}{C['reset']}")
    print(f"{C['ok']}[∞] Color → {color}{C['reset']}")
    print(f"{C['info']}[∞] Log updated → {TOKENS_LOG_PATH}{C['reset']}")
    print(f"{C['hdr']}[∞] Done.{C['reset']}")

if __name__ == "__main__":
    main()
